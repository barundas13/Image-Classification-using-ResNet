{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqVw_uwIAHb7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import glob\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BottleneckBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(BottleneckBlock, self).__init__()\n",
        "        width = out_channels // 4\n",
        "        self.conv1 = nn.Conv2d(in_channels, width, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(width)\n",
        "        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(width)\n",
        "        self.conv3 = nn.Conv2d(width, out_channels * 4, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_channels * 4)  # Ensure this is 4 * out_channels\n",
        "\n",
        "        self.downsample = downsample\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)  # Ensure matching channels\n",
        "\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "xyGs5UkSApXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super (ResNet, self).__init__()\n",
        "        self.inplanes = 64\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU())\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer0 = self._make_layer(block, 64, layers[0], stride=1)\n",
        "        self.layer1 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer2 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 512, layers[3], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * 4, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * 4),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * 4\n",
        "\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer0(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "VQumYqpIAxTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class givemodel():\n",
        "    def __init__(self, train_path, test_path, learning_rate, num_epochs):\n",
        "        self.train_path = train_path\n",
        "        self.test_path = test_path\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_epochs = num_epochs\n",
        "        self.transforms = transforms.Compose([\n",
        "                        transforms.Resize((224,224)),\n",
        "                        transforms.RandomHorizontalFlip(),\n",
        "                        transforms.ToTensor(), #0-255 to 0-1, numpy to tensors\n",
        "                        transforms.Normalize(mean =[0.6953, 0.6752, 0.6424],\n",
        "                            std=[0.1198, 0.1166, 0.1154],) #Determine mean and std of the dataset using Image_Normalization.ipynb\n",
        "                    ])\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        train_loader = DataLoader(\n",
        "            torchvision.datasets.ImageFolder(train_path, transform=self.transforms),\n",
        "            batch_size=64, shuffle=True\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            torchvision.datasets.ImageFolder(test_path, transform=self.transforms),\n",
        "            batch_size=32, shuffle=True)\n",
        "\n",
        "        self.model = ResNet(BottleneckBlock, [3, 4, 6, 3]).to(self.device)\n",
        "        self.optimizer = Adam(self.model.parameters(), lr=self.learning_rate, weight_decay=0.0001)\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.num_epochs = self.num_epochs\n",
        "\n",
        "        train_count = len(glob.glob(self.train_path + '/**/*.jpg')) #Change image format according to dataset\n",
        "        test_count = len(glob.glob(self.test_path + '/**/*.jpg'))\n",
        "\n",
        "        print(train_count, test_count)\n",
        "\n",
        "        best_accuracy = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "              # Evaluation and training on training dataset\n",
        "              self.model.train()\n",
        "              train_accuracy = 0.0\n",
        "              train_loss = 0.0\n",
        "              for i, (images, labels) in enumerate (train_loader):\n",
        "                    if torch.cuda.is_available():\n",
        "                        images = Variable(images.cuda())\n",
        "                        labels = Variable(labels.cuda())\n",
        "                    self.optimizer.zero_grad()\n",
        "                    outputs = self.model(images)\n",
        "                    loss = self.loss_function(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "                    train_loss += loss.cpu().data * images.size(0)\n",
        "                    _, prediction = torch.max(outputs.data, 1)\n",
        "\n",
        "                    train_accuracy += int(torch.sum(prediction == labels.data))\n",
        "\n",
        "              train_accuracy = train_accuracy / train_count\n",
        "              train_loss = train_loss / train_count\n",
        "\n",
        "              # Evaluation on testing dataset\n",
        "              self.model.eval()\n",
        "              test_accuracy = 0.0\n",
        "              for i, (images, labels) in enumerate (test_loader):\n",
        "                  if torch.cuda.is_available():\n",
        "                      images = Variable(images.cuda())\n",
        "                      labels = Variable(labels.cuda())\n",
        "                  outputs = self.model(images)\n",
        "                  _, prediction = torch.max(outputs.data, 1)\n",
        "                  test_accuracy += int(torch.sum(prediction == labels.data))\n",
        "              test_accuracy = test_accuracy/test_count\n",
        "              print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) + ' Train Accuracy: ' + str(\n",
        "                  train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
        "\n",
        "              # Save the best model\n",
        "              if test_accuracy > best_accuracy:\n",
        "                  torch.save(self.model.state_dict(), 'best_checkpoint1.model')\n",
        "                  best_accuracy = test_accuracy\n",
        "\n",
        "    def save_model(self, name_of_model):\n",
        "        torch.save(self.model, name_of_model)"
      ],
      "metadata": {
        "id": "maZzPScZBRXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"arunrk7/surface-crack-detection\")\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDOYlRBFZd8B",
        "outputId": "ae544a6f-2f7c-41fb-fa7c-ae8f129c53aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/arunrk7/surface-crack-detection?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 233M/233M [00:01<00:00, 123MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/arunrk7/surface-crack-detection/versions/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /root/.cache/kagglehub/datasets/arunrk7/surface-crack-detection/versions/1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RkVpVfT6oJzn",
        "outputId": "2152dc8f-13f8-4433-bae0-34d9f9f2a954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative  Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# Paths to the downloaded dataset and class folders\n",
        "source_dir = '/root/.cache/kagglehub/datasets/arunrk7/surface-crack-detection/versions/1'\n",
        "negative_dir = os.path.join(source_dir, 'Negative')\n",
        "positive_dir = os.path.join(source_dir, 'Positive')\n",
        "\n",
        "# Define paths for train and test directories\n",
        "train_dir = '/content/dataset/train'\n",
        "test_dir = '/content/dataset/test'\n",
        "train_negative_dir = os.path.join(train_dir, 'Negative')\n",
        "train_positive_dir = os.path.join(train_dir, 'Positive')\n",
        "test_negative_dir = os.path.join(test_dir, 'Negative')\n",
        "test_positive_dir = os.path.join(test_dir, 'Positive')\n",
        "\n",
        "# Create train and test subdirectories\n",
        "os.makedirs(train_negative_dir, exist_ok=True)\n",
        "os.makedirs(train_positive_dir, exist_ok=True)\n",
        "os.makedirs(test_negative_dir, exist_ok=True)\n",
        "os.makedirs(test_positive_dir, exist_ok=True)\n",
        "\n",
        "# Function to split and copy files\n",
        "def split_and_copy_files(class_dir, train_dest, test_dest, split_ratio=0.7):\n",
        "    all_files = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
        "    random.shuffle(all_files)  # Shuffle the files\n",
        "    split_index = int(split_ratio * len(all_files))  # Calculate split index\n",
        "\n",
        "    train_files = all_files[:split_index]\n",
        "    test_files = all_files[split_index:]\n",
        "\n",
        "    # Copy to train and test directories\n",
        "    for f in train_files:\n",
        "        shutil.copy(os.path.join(class_dir, f), os.path.join(train_dest, f))\n",
        "    for f in test_files:\n",
        "        shutil.copy(os.path.join(class_dir, f), os.path.join(test_dest, f))\n",
        "\n",
        "    return len(train_files), len(test_files)\n",
        "\n",
        "# Split Negative class\n",
        "train_neg_count, test_neg_count = split_and_copy_files(negative_dir, train_negative_dir, test_negative_dir)\n",
        "\n",
        "# Split Positive class\n",
        "train_pos_count, test_pos_count = split_and_copy_files(positive_dir, train_positive_dir, test_positive_dir)\n",
        "\n",
        "print(f\"Negative - Train: {train_neg_count}, Test: {test_neg_count}\")\n",
        "print(f\"Positive - Train: {train_pos_count}, Test: {test_pos_count}\")\n",
        "print(f\"Total images in Train: {train_neg_count + train_pos_count}, Total images in Test: {test_neg_count + test_pos_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7m758o2dp7LG",
        "outputId": "fd256897-19ea-4371-bf0d-0eb13a851998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative - Train: 14000, Test: 6000\n",
            "Positive - Train: 14000, Test: 6000\n",
            "Total images in Train: 28000, Total images in Test: 12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "      train_path= train_dir\n",
        "      test_path= test_dir\n",
        "      obj = givemodel(train_path, test_path, 0.001, 2)\n",
        "      obj.save_model(\"Custom_ResNet34_Model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fhwn1zgjBVSw",
        "outputId": "9f3e3ffb-5c96-40c2-c521-e1ab5ad4558c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28000 12000\n"
          ]
        }
      ]
    }
  ]
}